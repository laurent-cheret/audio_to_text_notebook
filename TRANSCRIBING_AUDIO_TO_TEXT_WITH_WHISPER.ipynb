{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "============================================================================\n",
        "AUDIO TRANSCRIPTION - Easy Setup Guide\n",
        "============================================================================\n",
        "\n",
        "HOW TO USE THIS (Simple 3-step process):\n",
        "\n",
        "1. RUN THE CODE\n",
        "   - (Detail: If you are able to select another environment with GPU instead of the standard CPU, this code will run significantly faster).\n",
        "   - Click the Play button (▶) on the left side of this code box\n",
        "   - OR press Shift + Enter on your keyboard\n",
        "   - Wait while it installs (you'll see text appearing below)\n",
        "\n",
        "2. UPLOAD YOUR AUDIO FILE\n",
        "   - A \"Choose Files\" button will appear\n",
        "   - Click it and select your M4A (or MP3, WAV) audio file\n",
        "   - Wait for it to upload (you'll see a progress bar)\n",
        "\n",
        "3. WAIT FOR TRANSCRIPTION\n",
        "   - The code will automatically transcribe your audio\n",
        "   - When done, your transcription will appear below\n",
        "   - A text file will automatically download to your computer\n",
        "\n",
        "THAT'S IT! No account needed, completely free.\n",
        "\n",
        "TIPS:\n",
        "- Longer audio files take more time (be patient!)\n",
        "- For better accuracy: Change \"base\" to \"small\" or \"medium\" below\n",
        "- For faster results: Change \"base\" to \"tiny\" below\n",
        "- You can transcribe multiple files - just run the code again\n",
        "\n",
        "============================================================================"
      ],
      "metadata": {
        "id": "_tOW7zJpQYHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "6ERnpCKsHAgJ",
        "outputId": "3c2065ee-b25f-42da-8b40-99288ba4b4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Whisper and dependencies...\n",
            "\n",
            "✓ Installation complete!\n",
            "\n",
            "Loading Whisper model (this may take a minute)...\n",
            "Available models: tiny, base, small, medium, large\n",
            "Larger models are more accurate but slower\n",
            "\n",
            "✓ Loaded 'base' model successfully!\n",
            "\n",
            "==================================================\n",
            "Click 'Choose Files' below to upload your M4A file\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b137fdee-b10a-432d-bd0d-1558d8ad06ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b137fdee-b10a-432d-bd0d-1558d8ad06ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Unpacking_AI_Fragility__Where_Human_Intuition_Beats_Machine_Bru.m4a to Unpacking_AI_Fragility__Where_Human_Intuition_Beats_Machine_Bru (1).m4a\n",
            "\n",
            "✓ Uploaded: Unpacking_AI_Fragility__Where_Human_Intuition_Beats_Machine_Bru (1).m4a\n",
            "\n",
            "Transcribing... (this may take a few minutes depending on file size)\n",
            "\n",
            "==================================================\n",
            "TRANSCRIPTION COMPLETE\n",
            "==================================================\n",
            "\n",
            " Welcome back to the deep dive. Our mission here is always the same. Take complex research, strip away the jargon, and hand you the essential, actionable insights. That's right. Today, we are wrestling with, well, probably the toughest abstract reasoning challenge out there in modern AI. That's the abstraction and reasoning corpus or RSC-AGI. Yeah, RSC-AGI. It's quite the beast. For anyone new to it, think of it as AI's ultimate test of few-shot generalization. Right, few-shot. Like learn from just a handful of examples. Exactly. And it looks deceptive, you know, just simple colored grids, an input example, an output example. But solving it, that needs real abstract reasoning. We're talking symmetry, gravity concepts, pathfinding, even complex counting sometimes. So it's not just one thing. It's like a whole suite of reasoning tests rolled into one. Pretty much. It's the closest thing we have really to a composite measure of, let's say, fluid intelligence for machines. And the progress has been impressive, hasn't it? I mean, we're seeing these complex AI systems, these ensemble solutions, B-A-R-C-L-P-N. They're getting close to human performance. They are. Some are approaching or even in specific areas, matching that average human baseline, which is somewhere around 60.2% accuracy. But that's not really the core mission for you, the listener, today. Right. Because you're coming at this from a totally different perspective. You're building this crowdsourcing platform, aiming to capture something else entirely. Yeah, you're after that unique, almost effortless brilliance of human intuition, the low compute kind. So your goal isn't just to see the AI succeed. It's actually the opposite. It's to figure out where it fails, where it's inefficient. Precisely. We need to understand the weaknesses of these current top approaches, you know, induction, transduction, test time training, those latent program networks. Understanding those weak points justifies why collecting the specific human data you're after is so critical. Exactly. Things like the first impressions people have, their initial hypothesis about the rule, the actual click-by-click solution process, even just how hard they felt the task was. That's the gold you need. So we're basically unpacking this AI frontier to pinpoint exactly where human thinking still runs circles around the machine. The sources we've looked at suggest that, well, every big AI win in ARC, hide some kind of hidden cost, right? Or some brittleness that humans just bypass. That's a great way to put it. And that brittleness, that fragility, it starts right at the very beginning. Like, how does the model even figure out what kind of problem it's looking at? Okay, let's unpack this then, starting right there. With that fundamental split, you mentioned, this dual nature of the current ARC solvers. Right. So the first, and maybe the most revealing sign of AI fragility here is that the best systems they can't actually commit to one way of reasoning. They're split, fundamentally split between two big paradigms. Yeah, what are they? There's induction, which is the sort of classic symbolic approach. And then there's transduction, which is more neural, more fuzzy direct prediction. All right, let's break those down. Induction first. What does that mean in like plain English? Induction is basically, think symbolic thought. The model tries to do program synthesis. It looks at the few examples you give it, the input output pairs, and tries to infer the actual hidden rule, the latent function. The transformation. Yeah, the transformation rule that explains all those examples. And often, this rule gets represented as, you know, actual Python code. Okay. Once it thinks it's found that code, it just applies that rule that function F to the test input to get the answer. The whole focus is on nailing down the rule itself. So induction is essentially figure out the hidden code. Got it. Exactly. Now, transduction, it flips that completely. How so? It's purely predictive. It's neural, usually a big neural network, maybe a fine tuned LLM. And it just skips the hard part of writing explicit code. Skips it. Yeah. It uses the training examples and the test input you give it. And it just directly predicts the final output grid. It's like, it's imitating the effect of the code, but without ever explicitly understanding the underlying symbolic rule. Okay. That's a pretty fundamental difference. It is. And what's genuinely surprising looking at the results from teams building these top systems, like BARC, is that even though they might use the same basic neural architecture underneath, like say, llama 3.1-8b instruct, which is actually pretty good with code. Right. These two methods, induction and transduction, turn out to be really strongly complementary. They solve different types of tasks. Wait, that seems odd. Doesn't that contradict earlier findings? Like, in neural programs, if this is didn't induction usually come out on top, stuff like robust fill. It does seem counterintuitive. Yeah. Historically, induction often proved better for generalization in program synthesis, but ARC tasks, they're just so diverse. Neither approach, neither general skill set can really handle the whole spectrum alone. So this forced split, this division of labor, that itself points to a weakness. A profound weakness, I'd say. Okay. So let's dig into that complementarity. What kinds of puzzles need that symbolic precision? Where does induction shine? induction really excels when the rule is deterministic, precise, and maybe needs composing multiple steps. Okay. Imagine a task where you have to say, count all the blue squares, then double that number, and then make a new row of yellow squares equal to that doubled count. Okay. Lots of steps. Explicit counting. Exactly. Explicit counting and the composition of several symbolic ideas. Or maybe a task with a very defined color mapping rule, like if a red square ever touches a blue one, they both disappear. That kind of precise computation, that's where the rigor of program synthesis of induction really pays off. Makes sense. So what about transduction, then? The fuzzy neural way? Where does it pick up the slack? Transduction tends to succeed on tasks that feel more perceptual, maybe harder to pin down perfectly with code. Visual stuff. Yeah, think visual pattern recognition, like recognizing an implied rotation in a shape, or judging if a complex shape is, I don't know, more vertical than horizontal to trigger some action. These tasks play more to the built-in spatial reasoning or visual processing strengths of the neural network itself. It almost sounds like the AI has to make this conscious metal level choice. Is this a math problem or a picture problem, which a human doesn't really do. We just see the solution. We just see it. Yeah. Is that the core weakness here? This forced distinction? I think it absolutely is. The very fact that the top performing systems, the state of the art, have to basically just trivially ensemble these two methods. That's proof of fragility right there. Trivially ensemble, meaning just stick them together. Yeah, pretty much. The system can't seem to integrate symbolic and perceptual reasoning smoothly or efficiently within one process. So it has to rely on this kind of cross-least sequential triage. Okay, describe that triage process. How does it work in practice like NBRC? The standard approach seems to be the system first throws a specific limited amount of test-time compute power at the induction engine. Okay. It starts sampling potential programs, you know, trying to find some code that perfectly fits all the training examples. And if it does find one or runs out of time? Right. If within that compute budget no satisfactory program pops out, maybe the problem's too fuzzy, maybe synthesizing the code is just too computationally hard, only then does the system basically give up on induction and switch gears. It deploys the transaction model as a fallback. A costly fallback. So that dependency really underscores it. Neither approach is robust enough by itself to handle the cheer abstract complexity of RCAGI. Exactly. And that brings us right back to your work to the data you're collecting. This is why getting those human first impressions and initial hypotheses is so incredibly valuable. How so? Because if we can capture that very first thought process that initial intuition, maybe we can start to understand why the human mind isn't forced into the split. Does our intuition somehow almost instantly synthesize a kind of proto program, something that satisfies both the precise and the fuzzy constraints all at once? That seamless integration. That integration is precisely what the AI is missing right now. And it's having to use this expensive computational crutch, this ensemble method, just to compensate. Okay. Let's shift gears a bit. We always hear about AI scaling, right? More data, bigger models, better performance. But with ARCGI solvers, there seems to be this huge contradiction, this data paradox. Yeah, they're really. ARC is supposed to be a few shot learning benchmark, learned from a few examples. Yet these top models, they're fine tuned on absolutely staggering amounts of data. You mentioned up to 400,000 synthetic problems. That's right. Up to 400K. It's kind of the dirty little secret of a lot of modern, few shot generalization benchmarks. How so? Well, the models aren't really generalizing purely from those four or five examples. They see it test time. What's happening is they're using massive pre-training and then this huge fine-tuning phase to basically, amortize the cost of inference later on. And impose the cost. Yeah, like pre-load all the possible reasoning patterns, those 400,000 synthetic problems, act like a giant bootstrapping library. Or maybe think of it as dream data that teaches the underlying LLM how to speak the language of ARC transformations fluently. Okay, but where does this massive Davis set even come from? 400,000 problems is a lot. You said it relies on a really thin slice of actual human input. It does. The whole process starts really small with human-created seeds. Seeds? Yeah, researchers manually write a pretty modest number, maybe say, 100 to 100 to 160 deterministic Python programs. Each one is designed to solve a specific ARC-like transformation concept. Okay, so human codes up like 150 basic ARC rules. Right. And each seed isn't just the code. It's a package deal. It has to contain three key parts. First, a natural language description of the concept like move the largest object down. Second, the transform grid function itself, that's the actual Python code that implements the rule. And third, critically, a generate input function. That part makes sure the system can create sensible input grids for that specific rule to work on. Gotcha. So the human seeds provide everything. The concept and words, the actual ruling code, and a way to generate examples, that's the entire conceptual starting point. Precisely. The humans are providing all the fundamental conceptual building blocks. Then the machines take over how? They use powerful LLM models like GPT-4, a mini or GPT-4, and they combine that with methods like RRAG, retrieval augmented generation. Okay, hold on, RRAG, what's that doing here? Right. So RRAG basically lets the LLM look back at the existing library of seed programs when it's trying to create a new problem. It uses the old ones to make new ones. Yeah, it uses in context learning. It might pull the natural language description from, say, seed A, move largest object down, and seed B, change color to blue, and then it tries to generate a new description that combines them, like, move the largest object down and change its color to blue. Then it synthesizes new code and new IO examples based on that remix description. It's like remixing the original human ideas over and over. Exactly. It's like a chef who knows 100 base recipes and is told, okay, make something new that combines the spice profile of recipe A with the cooking technique of recipe B. It generates variations endlessly. Leading to 400,000 problems. But here's weakness, hashtag one, right? The conceptual bottleneck. If they have this enormous data set, why isn't that enough? Why can't they solve everything? Because the key finding, the really crucial insight from the research, is that performance improvements saturate very quickly when you simply increase the number of human created seeds. Meaning adding more human coded rules stops helping after a certain point. Pretty much. Yeah. Once you've provided maybe those initial 150, 160 core concepts, things like basic rotation, simple counting, maybe some pathfinding ideas, color filling. After that, adding more manually coded seeds gives you diminishing returns, barely moves the needle on overall performance. Wow. So after that threshold, basically all the core concepts that are, let's say, easy for humans to think of and code up in Python have already been introduced. That's the interpretation. Yes. To get further performance gains after that point, you don't add more human seeds. You have to exponentially increase the compute budget for generating more synthetic data. So just more remixing more remixing more permutations of the existing concepts. The model becomes an incredible remix artist, a master combiner, but it struggles to be an inventor. It can't easily introduce a genuinely new conceptual prior, something fundamentally different that wasn't hinted at in those initial 100 odd human written seeds. That feels like a massive gap. The AI is brilliant at iterating on what it already knows, but it's pathologically bad at making a real conceptual leap when faced with something truly unfamiliar. That seems to be the case. And again, this is exactly why the data you're collecting on the humans initial hypothesis is potentially so powerful because because when a human encounters a genuinely novel ARC problem that first mental articulation of the rule that aha moment, that's the raw ingredient. That's the conceptual leap that the synthetic program generator missed probably because the original human programmers just didn't happen to think of encoding that specific type of prior in the initial seed set. So collecting that first hypothesis is like capturing the mechanism of conceptual invention itself. You think of it that way. Yeah. We're trying to bottle that lightning. And just to tie this section off these models like BRRC, they're built on top of these code savvy LLM's, right? Like llama 3.1-8B instruct. Correct. They specifically choose base models pre-trained on source code because that gives them a head start. And then they fine-tune them using specific loss functions, either to generate the symbolic functions for induction or to directly predict the output grid test for transduction. Right. But the ultimate quality, the conceptual breadth of what those models learn seems fundamentally capped by the breadth of those initial human sparks, those seed programs. It's like a conceptual ceiling imposed by the limits of the initial human input. Exactly. No amount of computational remixing can invent the color yellow if you only ever gave the system red and blue to start with. Okay. So we've seen issues in the fundamental approach and in the training data. But the problems don't stop there, right? Even during inference, when the AI is actually trying to solve a new ARC task, it seems to rely on these, well, they feel like computationally expensive patches or workarounds like test time training or TTT. Oh, yeah. TTT is a big one. And it definitely highlights a major weakness in terms of, well, efficiency and true generalization. So what is it? Instead of just running the model, it changes itself. Sort of. Yeah. Instead of just doing a standard forward pass input in, prediction out the model temporarily updates a small part of its own parameters while it's solving that specific test task, it uses the handful of examples provided for that problem to quickly adapt itself on the fly. Wait, how is that even feasible? We're talking about huge models, right? How can you afford to tweet parameters for every single little ARC problem you encounter? Isn't that incredibly slow and expensive? It would be if you were updating the whole model, but that's where these parameter efficient fine-tuning techniques come in, especially Loreal low-rank adaptation. Loreal, right. That lets you freeze most of the big model. Exactly. You freeze the vast majority, maybe billions of the main models weights. And you only train these tiny lightweight adapter modules that sit alongside the main weights. These adapters have far fewer parameters so they can be updated really quickly, even just using the few examples from the current ARC task. And this isn't just a nice to have, it's essential. Oh, it's absolutely essential for getting top scores with the Transduction approach. The study show TTT can take a model's accuracy from a decent fine-tuned baseline, maybe around 18% and push it way up, closer to 50%, sometimes even higher. It's a massive performance boost. Wow, so the model is basically doing this rapid, almost emergency re-learning for every single new task it sees. That's a good way to think about it, yeah. But how do they structure that? If you only have, say, three or four input output examples for a task, how do you train on those at test time without just instantly overfitting and memorizing them? Right, that's a key challenge. They use a clever but also quite complex technique. It's often called a leave one-out strategy. You've won out. Yeah. So imagine a task gives you three example pairs, A, B, and C. The model doesn't just look at A, B, and C together. Instead, it uses examples B and C to try and predict A. Then it uses A and C to try and predict B. And finally, it uses A and B to try and predict C. Ah, so it takes the few training examples it has and turns each one into a kind of mini-synthetic test case within the test time adaptation. Precisely. It forces the model to learn the underlying rule or transformation that connects the pairs rather than just memorizing the pixel patterns of A, B, and C. Sure. And there's more. The loss function they use during this TTT step isn't just calculated on the final prediction for the actual test input. It's calculated over all the outputs, the predictions for the held-out examples, like predicting A from B and C, and the prediction for the final test input. Why do they? It encourages the model to learn the rule more systematically, to find parameters that work consistently across all the available evidence. And one more critical point for ARCAGI specifically. The research generally shows that learning a separate task-specific lore-at-adapter for each individual problem that usually outperforms trying to use just one single adapter shared across all problems. Which means even more compute costs per problem, right? Because you're training a unique adapter every time. Exactly. More compute, more complexity. It all points to this profound lack of, let's call it out of the box generalization. The model needs this intense problem-specific handholding just to perform well. Which brings us neatly to weakness hashtag to something you touched on earlier. Geometric rigidity. Even with TTT, these neural models seem to struggle with basic spatial reasoning unless you add even more external machinery. That's right. The base neural-transduction model, even after fine-tuning in TTT, often shows these weird internal biases. Like, it might be really good at spotting horizontal patterns or symmetries, but pretty bad at recognizing the same pattern if it's rotated vertically or diagonally. It lacks rotational or reflectional invariance. So it's rigid. How do they fix that? Well, they basically have to brute force it. The system is forced to generate multiple candidate predictions, but it does so by using a whole battery of invertible geometric transformations. Okay, walk me through an example. What does that look like? All right. Imagine the input grid has some pattern. The system first gets a prediction for that original, untransformed grid from the TTT-adapted model. Okay, prediction one. Right. But then it takes the input grid, rotates at 90 degrees clockwise, and feeds that rotated input to the model to get another prediction. Then it rotates the original input 180 degrees, gets a prediction, 270 degrees, another prediction. Then it flips the original input horizontally, gets a prediction, flips it vertically, gets another. Sometimes they even permute the colors systematically and get predictions for each color permutation. Whoa. So they could end up generating dozens of different candidate output grids for a single problem. Easily. Dozens. All based on these different geometric disguises of the original input. Okay, but if you have dozens of candidates, how on earth do you pick the right one? Do they just average them? No, averaging pixel grids usually gives you mush. They need something much more structured and again, more complex, a hierarchical voting scheme. hierarchical, what does that mean? It means they don't just throw all say 30 predictions into one big vote. First, they group the predictions based on the transformation used. So all the predictions from the 90 degree rotations vote amongst themselves, all the predictions from the horizontal flips, vote amongst themselves, and so on. They find the most consistent or highest confidence prediction within each transformation group. Okay, so internal votes first. Right. Then the winners, the top candidates from each of those internal group votes move on to a final global vote to determine the single best overall prediction. That sounds incredibly complex and computationally heavy just to handle basic rotations and flips. It is. And the very reliance on this elaborate external post processing voting system is strong evidence that the underlying neural model isn't naturally a robustly permutation invariant or transformation invariant. They have to bolt on this complex mechanism to force it to behave as if it were another computational crutch. Okay, let's swing back to the other approach induction, the code writing one that's not without its own problems, right? You mentioned the programmatic search weakness. Definitely. Induction has its own efficiency Achilles heel. It's accuracy scales very strongly, almost linearly, with how much compute you dedicate to it at test time. Specifically, how many candidate programs you sample? How many are talking? Well, top systems might sample upwards of 20,000 potential Python programs in their attempt to find one that fits the few input output examples provided for the task. 20,000 programs for one ARC task. Can be. Yeah. But here's the catch. This massive search effort introduces a significant problem the issue of false positives. False positives. Yeah. Meaning programs that look right but aren't. Exactly. Remember, the goal of the search is to find a program that correctly transforms all the given training input examples into their corresponding training output examples. A false positive is a program that achieves this perfectly. It works for all the training pairs, but when you apply it to the actual test input, it gives the completely wrong answer. Can you give an example of how that might happen? Sure. Let's say the training examples all happen to show a single red square located at grid position two, three, moving one step to the right to three, three. The true underlying rule is simply move the red square one step right. Okay. But during the search, the system might generate a different program. Like if a red square is at position two, three, then move it to three, three, LSE delete the entire grid. Now, because in all the training examples, the red square was at two, three, this incorrect program perfectly fits the training data. It looks correct. Ah, but then the test input comes along and maybe the red square starts at four, five. Exactly. And the false positive program following its faulty ELSE condition just deletes the grid entirely. It fails dramatically on the test case, even though it passed all the training checks. It overfitted to the specifics of the few examples. And how common are these false positives? One study reported that around 9% of the programs sampled during the search programs that perfectly fit the training examples were actually false positives that failed on the hidden test case. 9% because a huge amount of wasted effort in that massive 20,000 program search. It's a staggering inefficiency, right? The AI is burning immense amounts of computation, generating thousands upon thousands of candidate solutions, and a significant chunk of that effort is producing brittle, overfitted junk programs. Fitting just a few examples is fundamentally insufficient for robust generalization. And this loops right back to your data collection, doesn't it? Yeah. Collecting the human click-by-click solutions and their final thought process. Precisely. A human typically doesn't mentally simulate 20,000 possible rule variations and then run hierarchical vote on them. They usually execute a relatively short high-confidence sequence of actions or mental transformations. Capturing that efficient, high-confident solution path is like capturing the antidote to the AI's current inefficient brute force search problem. You're defining what efficient problem-solving actually looks like. Okay, so if a big part of the problem, especially for these programmatic or induction methods, is that standard approaches like just asking an LLM to write code lack a really structured deep search mechanism. How are researchers trying to fix that? How do they try to bake in that more deliberate system to kind of thinking? This leads us to things like the latent program network, OPN, right? Exactly. OPN is a really quite clever attempt to bridge that gap. To marry the predictive power you get from neural networks with the need for structured, almost symbolic-style reasoning in search. How does it work? You mentioned it doesn't generate explicit code. Right. Instead of outputting, say, discrete Python code, LPN tries to map the task specification, meaning the input output example pairs to a continuous latent program space. A latent space. So not code, but like a mathematical representation of the program's idea, a dense vector. That's a perfect way to think of it. Yeah. It's a point in a high-dimensional concept space that represents the network's internal understanding of the required transformation role. And the whole LPN architecture is designed explicitly as an induction machine, but one that tries to mimic human thought. A quick intuition followed by slower, more careful deliberation. Okay, so it has steps. Step one is the intuition part. Right. Step one is the encoder. You can think of this as the fast intuitive system one guess. It takes the IO pairs for the task and very quickly provides an initial estimate, usually the mean of the latent vector, seven. It's like the network's first draft guess of the program's core concept. And you hinted this first guess might not be great. As we'll see, especially for tasks that are a bit novel or out of distribution, this initial intuitive guess is, uh, well, pretty terrible actually. Okay, so then comes step two. Deliberation. The latent optimization. This is the system two search part. This is the critical and computationally more expensive part. It takes that initial guess zealors and tries to refine it into a much better latent program. Let's call it zeal deer. It does this using gradient-based search specifically. They often use gradient-assent GA directly within that continuous latent space. Gradient-assent on the idea of the program. How does that work? The search aims to adjust zealors in a direction that maximizes the likelihood that the program represented by zealor correctly explains all the provided training examples. It's literally performing optimization on the conceptual representation of the rule itself, trying to make it fit the evidence better. Okay, that's cool. And then step three. Step three is the decoder. Once you have the refined, optimized latent programs zealor, the decoder simply executes that latent program on the new unseen test input grid to generate the final predicted output grid. And the research on LPN you said really proves that this search step, this deliberation is absolutely essential. Relying on just the first guess, system one only, doesn't cut it. The data is incredibly clear on this. Just using the encoder's initial intuitive guess performs very poorly, especially on harder tasks. But running that gradient ascent in the latent space, it vastly outperforms just doing random search in that space. And it can often recover near-perfect accuracy on tasks where the initial guess was completely wrong. So it really demonstrates the need for some kind of structured iterative refinement, a computational version of thinking harder about the problem. Absolutely. It shows that for complex abstract reasoning, you really need something analogous to human deliberation, not just instant intuition. Okay, but here's where it gets really fascinating and maybe a bit depressing for the AI. Despite successfully building in this powerful search mechanism, LPN still runs headfirst into weakness hashtag three, the out of distribution generalization fragility. Yeah, this is where we see the limits again, especially when dealing with conceptual mobility. Researchers specifically tested LPN by training it on one type of problem distribution, and then testing it on tasks that were significantly out of distribution or ODE. What kind of ODE shifts are we talking about? Things that change a core parameter of the concept. For example, maybe they trained LPN extensively on ARC tasks involving creating patterns or paths that only covered say 50% of the grid squares, relatively sparse patterns. Okay. Then they tested it on problems requiring similar types of pattern formation, but where the final pattern needed to cover 100% of the grid, like filling it in completely, making it dense, that shift in the density parameter makes it oodd. Right, a conceptual parameter shift. Yeah. So what happens when LPN encounters that novel density task? How does its initial guess fair? The results are stark. The model's initial prediction accuracy, just using that first intuitive guess from the encoder, the mean latent lever drops to almost 0%. Near total failure. Zero. So it fundamentally failed to generalize the core concept like path transformation from the sparse examples it was trained on to the dense examples it was tested on. It seems so. It just couldn't make that zero shot generalization leap across even what seems like a relatively minor conceptual shift in a parameter like density. That's that's a complete failure of the kind of quick flexible generalization humans seem to do all the time. But then these expensive search comes to the rescue. This is the really interesting part. Yes. The researchers found that even though the initial guess was useless, if they then invested the computational resources to run that gradient ascent search at test time, maybe doing 100 steps of GA to refine the latent program. Seven wall. The model could dramatically recover performance. It could climb back up to achieve accuracy as high as 88% even on those strongly OD density tasks. Okay, wait. So the model can ultimately solve the novel task, but only by burning a significant amount of compute during the test itself, specifically optimizing its internal concept representation until it finally fits this new unexpected data distribution. That's exactly what it suggests. It has the latent capacity to represent the solution, but it can't access it intuitively or generalize to it directly. It needs this costly task specific optimization process to adapt on the fly, whereas a human might just see the dense pattern and mentally adjust their density filter or rule parameter almost instantly. Possibly yes. It confirms that even these quite sophisticated induction methods like LPN really struggle with conceptual novelty or parameter shifts. They require this expensive bespoke optimization at test time to compensate. It's another computational crutch, really the inverse of human cognitive efficiency, which again brings it back to your data collection. This strongly suggests that by collecting data on whether a person needed to rethink their hypothesis or maybe tracking moments where their solution path changes direction, you're essentially mapping the landscape of these OD failures for the AI. You are. You're defining precisely those moments where the current AI would have to grind through a hundred steps of grading descent, burning precious compute, just to catch up to what might be for a human, just a couple of seconds of mental adjustment or reevaluation. That gap is huge. Okay, so analyzing how the AI scores across the different ARC AGI problems reveal something really profound. And maybe this is the single most important justification for your crowdsourcing plan. It's what we can call the difficulty mismatch. This is weakness hashtag four. And it seems fundamentally rooted into the different kinds of prior knowledge or biases that humans and AI bring to the table. Difficulty mismatch. Yeah. Okay, tell me more. How does it show up in the scores? You mentioned researchers categorize the ARC problems based on how hard humans found them. That's right. They looked at the human success rates on the validation set, ordered the problems from easiest, highest human success to hardest, lowest human success. And when you plot the AI's performance, whether it's induction, transduction or the BRC ensemble against that human difficulty ranking, you see this striking almost perfectly inverted curve, inverted, meaning the AI does better on the problems humans find hardest. Exactly. The models actually surpass average human performance on roughly the hardest 20% of the official ARC problems. That seems backwards. Why would that be? Well, think about what might make an ARC problem hard for a person. It might involve very precise, fiddly spatial manipulations, lots of steps, maybe non-intuitive transformations, things that are visually confusing or hard to track mentally. Right. Things that overload our working memory or visual processing. Precisely. But those same complex, fiddly multi-step transformations, they can often be represented surprisingly simply and concisely as deterministic Python code. They might involve iterating through coordinates, applying very specific rules based on neighbor counts or exact positions, computationally precise, but perceptually nightmares for us. Okay, give me a hypothetical example of a task like that. Hard for a human easy to code. All right, imagine a task requires you to take the input grid, reflect it across the main diagonal, then count the number of green squares in the third column, and finally swap the color of exactly 17 specific pixels based on a predefined list of their x, y coordinates. Yeah, that sounds awful to try and visualize or do by hand. Exactly. It's a nightmare for human intuition and visual processing. But a deterministic Python script, it can execute those precise steps flawlessly and efficiently. The AI's program synthesis engine induction might actually find the minimal code representation for these kinds of problems quite effectively. Okay, so the AI excels at the computationally precise but perceptually confusing tasks. So, conversely, where do the models fall down? They severely underperform on the tasks that humans find easiest. The problems that people solve almost instantly with near-perfect accuracy, often with just a glance. Why would those be hard for the AI? It likely comes back to priors. These easy tasks often seem to rely on innate human perceptual abilities or very quickly learned visual heuristics, things like recognizing subjective contours, completing patterns based on gestalt principles, understanding basic object persistence or containment almost intuitively. These fundamental priors might not have been well represented or even present in those initial 160 human written seed programs used to generate the synthetic training data. So the AI just never learned those basic foundational visual common sense rules. Or at least didn't learn them robustly enough. The synthetic data generator, being based on explicit code, might have struggled to capture these more implicit, perhaps harder to articulate human intuitions. This is a huge insight, then. The core AI weakness isn't necessarily in handling super complex logical steps. It's in the effortless conceptualization of rules that seem simple or obvious to us the ones we take for granted. That seems to be a major part of the story. Yes. If the AI has to spin up its complex machinery, run a massive program search, or engage TTT and hierarchical voting just to solve a basic pattern completion task that a human gets in three seconds using raw intuition, well, that efficiency gap is enormous. It's a sign that the AI is missing something fundamental about how efficient reasoning works. And this inefficiency, this brittleness, it isn't just confined to these academic research systems, is it? It seems to pop up even when looking at the cutting edge, the big commercial models. That brings us to weakness, hashtag five. Commercial model, compute inefficiency. Right. There was a recent study that specifically analyzed how some of the big commercial frontier models, like open AI's models, using their different reasoning settings, performed on ARC tasks. And it revealed some serious issues with computational cost and reliability. What kind of issues? Well, for one, when they tried using the designated high reasoning settings on these models, the models frequently just failed to respond. They timed out or they refused to answer for the ARC tasks. Timeed out. Suggesting that whatever complex reasing process was happening under the hood was just too slow or resource intensive. That's a strong possibility, yes. Yeah. It suggests that deploying that level of high compute reasoning, at least in its current form, might be prohibitively expensive, or perhaps too brittle and prone to getting stuck in loops for many abstract tasks like ARC. Wow. And the inefficiency didn't stop there. Did there was something about token usage? Yeah. This was really pathological when they compared the performance on easy ARC tasks. The ones humans solve quickly using say a medium reasoning versus a high reasoning setting. The high reasoning models consistently used more compute resources measured in tokens than the medium reasoning models to solve the exact same easy problems. And often, there was no improvement in accuracy. That's like using a sledgehammer to crack a nut. The extra reasoning complexity wasn't helpful. It was just overhead. Exactly. It was just wasted computational effort making the process slower and more expensive for no benefit on those simpler tasks. And the most damning observation about token usage. The analysis showed that pretty consistently across the board, when the models get the answer wrong, they tended to consume more tokens than when they got the answer right. More compute used on failures. How does that make sense? It suggests that the incorrect reasoning paths are perhaps longer and more convoluted. The AI might explore deeply down a complex but ultimately flawed line of thought, trying various sophisticated steps only to reach a dead end and fail, having burned a ton of tokens in the process. A correct perhaps more elegant or generalized solution path seems to be computationally cheaper and quicker. That's deeply inefficient. Which leads us naturally away from just static pattern recognition like the original ARC. And towards the future direction, the ARC prize foundation is pushing now these interactive reasoning benchmarks, IRBs, like ARCAGI3. Right. The focus is shifting. It's moving towards multi-step skill acquisition, often in environments that feel more like simple games. The goal isn't just to recognize a pattern in one shot. It's about interacting with an environment, gathering information, forming a strategy, and then executing it effectively. And the key metric is changing too. Yes. The new core metric is becoming action efficiency. It's not just about if you can solve the task, but how quickly and with how few actions you can convert the information you gather from the environment into a successful minimal strategy. They want the shortest, smartest solution path. And predictably, this is where that foundational weakness in AI efficiency becomes even more glaringly obvious. Be hugely magnified. Yes. We see it clearly in the preview competitions. Humans tend to explore the interactive environment briefly, maybe poke around a bit, form a plan, and then execute a short sequence of actions successfully. And the AI agents, like Stochasticoose, wasn't that one? Yeah. Agents like that often struggle immensely. They might resort to just trying actions almost randomly, pure brute force exploration loops, or they might require hundreds, even thousands of actions, to eventually stumble upon a solution for a level that a human figures out and solves and say, a dozen steps. So the fundamental weakness persists, maybe even worsens. The AI is just really poor at efficiently converting new information gathered through interaction into a working compact strategy. That seems to be the core challenge moving forward. And looking at this whole picture, the AI failing on easy tasks, wasting compute on wrong answers, displaying terrible action decisions in interactive settings, it just hammers home why collecting that human experiential data is so necessary. Absolutely. By capturing the perceived difficulty of a task for human, you're flagging the problems where AI priors might be weak. By capturing the initial hypothesis, you're getting at that missing conceptual spark. And by recording the click-by-click solution path, you're literally modeling the human capacity for rapid, low-compute, efficient generalization, and planning that the current AI systems just can't seem to replicate internally. Hashtag tag outro. Okay, so let's try and synthesize this. We've uncovered several core weaknesses in the current AI approaches to ARCAGI. And crucially, these map directly onto the kinds of data points that your crowdsourcing platform is setting out to collect. Right, let's list them. First, we saw that conceptual division. The AI systems seem forced to rely on these costly somewhat brittle ensembles. They have to manually choose between the symbolic, precise mode of thinking induction. And the more fuzzy neural perceptual mode transduction. They can't seem to integrate these two ways of understanding the world efficiently within a single system. It proves they lack a kind of unify cognitive conceptualization that humans might possess. Second, big weakness. Generalization fragility. Fish showed up everywhere, whether it was LPN needing that expensive gradient descent search at test time. Or the transduction models needing test time training and those complex hierarchical voting schemes just to handle basic geometric variations. The takeaway is that even highly trained models require these heavy task specific adaptation mechanisms during inference just to cope with minor conceptual novelty or shifts slightly outside their training distribution. They lack that robust immediate zero-shot generalization ability. And third, related to that, the massive efficiency gap. We saw this in the commercial models burning far more tokens on incorrect reasoning paths. And also in the new interactive benchmarks, where AI agents need potentially thousands of actions compared to a few efficient human moves. The current AI approaches seem structurally wasteful and computationally inefficient compared to human problem solving. Okay, so connecting these weaknesses directly to your data collection plan for the listener. By collecting human data on first impressions and those crucial initial hypotheses. You are directly targeting that conceptual invention, that spark of insight, that the current synthetic data generators seem to miss. You're capturing the priors, the AI lacks. And by collecting the detailed, click-by-click solutions. You're essentially creating the gold standard benchmark for action efficiency and planning. You're showing what a direct non-wasteful solution path actually looks like. And finally, by capturing perceived difficulty in those moments where a person needed to rethink their hypothesis. You're effectively mapping the ODO failure modes. You're pinpointing the exact situations where current AI models get brittle and are forced to burn massive amounts of compute on adaptation mechanisms, like greeting to scent, just to keep up. So what's the big picture take away here? It seems the AI models, perhaps counterintuitively, are currently better at solving the ARC problems that are easy to describe in precise code, but are perceptually complex or confusing right? They struggle with the basics, the intuitively obvious pattern, which leaves us with a final provocative thought for you, the listener, to mull over. Given this weird inversion AI excelling where code is simple but perception is hard, and failing where perception is simple, what fundamental cognitive shortcut, what innate or deeply learned human prior, allows us to solve a complex visual pattern so effortlessly, not as a complex program, not as a fuzzy neural prediction, but as an immediate integrated low compute conceptual leap. That answer, whatever it is, it seems to be locked within precisely the kind of human intuition and process data that you're setting out to harvest. Good luck capturing it.\n",
            "\n",
            "✓ Transcription saved to: Unpacking_AI_Fragility__Where_Human_Intuition_Beats_Machine_Bru (1)_transcription.txt\n",
            "\n",
            "Downloading transcription file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f6411c1c-5433-41c8-9cb1-09fdf3e0ec64\", \"Unpacking_AI_Fragility__Where_Human_Intuition_Beats_Machine_Bru (1)_transcription.txt\", 46232)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ All done! Check your downloads folder for the transcription.\n"
          ]
        }
      ],
      "source": [
        "# Audio Transcription with Whisper\n",
        "# This notebook transcribes M4A (and other audio formats) to text using OpenAI's Whisper\n",
        "\n",
        "# Step 1: Install required packages\n",
        "print(\"Installing Whisper and dependencies...\")\n",
        "!pip install -q openai-whisper\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import whisper\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"\\n✓ Installation complete!\")\n",
        "\n",
        "# Step 3: Load Whisper model\n",
        "print(\"\\nLoading Whisper model (this may take a minute)...\")\n",
        "print(\"Available models: tiny, base, small, medium, large\")\n",
        "print(\"Larger models are more accurate but slower\\n\")\n",
        "\n",
        "# Choose model size (base is a good balance)\n",
        "model_size = \"base\"  # Change to \"small\", \"medium\", or \"large\" for better accuracy\n",
        "model = whisper.load_model(model_size)\n",
        "\n",
        "print(f\"✓ Loaded '{model_size}' model successfully!\")\n",
        "\n",
        "# Step 4: Upload your audio file\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Click 'Choose Files' below to upload your M4A file\")\n",
        "print(\"=\"*50)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "print(f\"\\n✓ Uploaded: {audio_file}\")\n",
        "\n",
        "# Step 5: Transcribe the audio\n",
        "print(\"\\nTranscribing... (this may take a few minutes depending on file size)\")\n",
        "result = model.transcribe(audio_file)\n",
        "\n",
        "# Step 6: Display results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRANSCRIPTION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n\" + result[\"text\"])\n",
        "\n",
        "# Step 7: Save transcription to a text file\n",
        "output_filename = os.path.splitext(audio_file)[0] + \"_transcription.txt\"\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "print(f\"\\n✓ Transcription saved to: {output_filename}\")\n",
        "\n",
        "# Step 8: Download the transcription\n",
        "print(\"\\nDownloading transcription file...\")\n",
        "files.download(output_filename)\n",
        "\n",
        "print(\"\\n✓ All done! Check your downloads folder for the transcription.\")"
      ]
    }
  ]
}